---
title: Notes on matroid clustering experiments
---

```{r echo=F}
knitr::opts_chunk$set(
    echo = FALSE, message = FALSE, warning = FALSE,
    fig.width = 10, fig.height = 10
)
```

## Execution time/quality tradeoff

This set of experiments eaims at exploring the relationship between
the running time of an algorithm and the radius of the returned clustering,
under different parameterizations.

The following algorithms are tested:
- `Random`: select a random maximal independent set as 
   the set of cluster centers.
- `ChenEtAl`: this is the baseline 7-approximation algorithm, which takes
  no parameters. We run it only on a sample of the full dataset.
- `SeqCoreset`: the sequential coreset construction, featuring as the only
  parameter $\tau$, which controls the size of the coreset
- `StreamCoreset`: constructs the coreset in streaming, with a single parameter
  $\tau$ controlling the coreset size, and hence the working memory
- `MRCoreset`: builds the coreset in parallel. Takes two parameters:
  $\tau$ which controls the size of the coreset, and the number of parallel
  workers to use, which ranges from 2 to 16.
  In these experiments, $\tau$ controls the size of the _composed_ coreset, so for
  $w$ workers each worker builds a the coreset around a clustering with $\tau/w$ centers.

For coreset-based algorithms, we set $\tau \in [512, 1024, 2048, 4096]$.
The number of outliers is fixed at $1\%$ of the size of the dataset.
We run on both the full Wikipedia dataset, and on a sample of 50000
points, which is the largest size that can be handled by
`ChenEtAl`^[which requires to pre-compute the distance matrix, thus requiring quadratic space].

As for the matroid constraint, the points in the dataset belong to one or more
of 100 categories, numbered from 0 to 99. 
In this first set of experiments, we impose a transversal matroid constraint
on either a set of the first 10 categories, or on all categories. These two constraints
are denoted in the plots as `Rank10` and `Rank100`.

The points in the dataset are derived from
Wikipedia pages using [GloVe](https://nlp.stanford.edu/projects/glove/).
As a distance metric we can use either the euclidean distance or the 
cosine distance.
In this set of experiments we consider both.

Each experiment is repeated 5 times, and the following plot
reports the averages. On the left we have the full datasets,
on the right we have the samples.
Hovering with the mouse on the dots shows the radius, time and parameter configuration.
On top we have datasets using the euclidean distance, at the bottom those using the cosine distance.

```{r}
girafe(
    ggobj = drake::readd(plot_tradeoff)
)
```

As expected, `ChenEtAl` is the slowest algorithm and, somewhat surprisingly, is not the most accurate one,
even though all coreset-based algorithms use it as a subroutine to compute their output.
The other surprising thing is the performance of `Random`.
While it is obvious that it should be the fastest, since it does very little work, the quality of
random clusterings is surprisingly good.
So good, in fact, that it outperforms some configurations of the coreset-based algorithms, especially
on the full dataset under euclidean distance.
One can easily see that an heuristic that simply draws some random independent sets and then keeps the maximum 
would get very good results, even though with no guarantees.

As for the coreset-based approaches, we have that `SeqCoreset` and `MRCoreset` provide solutions of comparable quality

One could also think about drawing some random independent sets on the coresets and compare their radius with the
one obtained with `ChenEtAl`, keeping the best.

## Time breakdown

The following plots break down the time in two components: time to build the
coreset and time to compute the solution on it. The main takeaway is that
computing the final solution dominates the running time, so much that it
makes leveraging parallelism harmful. When running the MapReduce coreset, in
fact, using more processors entails larger corsets (even if we keep the final
$\tau$ constant) because each cluster will be represented by an independent set.

The number at the base of each column is the coreset size, averaged across
all the runs with the same parameters.

```{r fig.height=8}
girafe(ggobj=drake::readd(plot_time))
```

## Time for building the coreset only

What is clear from the plots below, by comparing the coreset construction
times on the full dataset and on the sample is that the cost of computing
larger independent sets (rank 100 vs rank 10) as part of the coreset
construction completely dwarfs the benefits of parallelism. Why is that the
case?

```{r fig.height=8}
girafe(ggobj=drake::readd(plot_time_coreset))
```

## Discussion points

- The problem with the streaming algorithm, compared with the sequential
algorithm, is that it needs to run the independent set oracle for each insertion
that falls within radius. A simple solution is to stop trying to add as soon as
we get a maximal independent set. However, this would require to inspect the
entire set before running the streaming algorithm.
- The time to compute the independent set is also the dominating factor when running the
MapReduce algorithm: after running k-center locally most of the time is spent in getting 
maximal independent sets from each cluster
- I think there was a bug in the Streaming, algorithm, which I corrected.
It might be that a point which cannot take part in any independent set becomes a center
when it is first considered by the algorithm. Then, we cannot simply try to add new points
to it to augment the independent set, because it is not one to begin with.

