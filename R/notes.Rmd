---
title: Notes on matroid clustering experiments
---

```{r echo=F}
knitr::opts_chunk$set(
    echo = FALSE, message = FALSE, warning = FALSE,
    fig.width = 10, fig.height = 10
)
```

## Execution time/quality tradeoff

**Caveat**: the observations on the `Random` algorithm might hold only for this very low-rank
matroid we are using. We should test with a higher-rank matroid.

This set of experiments eaims at exploring the relationship between
the running time of an algorithm and the radius of the returned clustering,
under different parameterizations.

The following algorithms are tested:
- `Random`: select a random maximal independent set as 
   the set of cluster centers.
- `ChenEtAl`: this is the baseline 7-approximation algorithm, which takes
  no parameters. We run it only on a sample of the full dataset.
- `SeqCoreset`: the sequential coreset construction, featuring as the only
  parameter $\tau$, which controls the size of the coreset
- `StreamCoreset`: constructs the coreset in streaming, with a single parameter
  $\tau$ controlling the coreset size, and hence the working memory
- `MRCoreset`: builds the coreset in parallel. Takes two parameters:
  $\tau$ which controls the size of the coreset, and the number of parallel
  workers to use, which ranges from 2 to 16.
  In these experiments, $\tau$ controls the size of the _composed_ coreset, so for
  $w$ workers each worker builds a the coreset around a clustering with $\tau/w$ centers.

For coreset-based algorithms, we set $\tau \in [512, 1024, 2048, 4096]$.
The number of outliers is fixed at $1\%$ of the size of the dataset.
We run on both the full Wikipedia dataset, and on a sample of 50000
points, which is the largest size that can be handled by
`ChenEtAl`^[which requires to pre-compute the distance matrix, thus requiring quadratic space].

As for the matroid constraint, the points in the dataset belong to one or more
of 100 categories, numbered from 0 to 99. 
In this first set of experiments, we impose a transversal matroid constraint
on the set of the first 10 categories.

The points in the dataset are derived from
Wikipedia pages using [GloVe](https://nlp.stanford.edu/projects/glove/).
As a distance metric we can use either the euclidean distance or the 
cosine distance.
In this set of experiments we consider both.

Each experiment is repeated 5 times, and the following plot
reports the averages. On the left we have the full datasets,
on the right we have the samples.
Hovering with the mouse on the dots shows the radius, time and parameter configuration.
On top we have datasets using the euclidean distance, at the bottom those using the cosine distance.

```{r}
girafe(
    ggobj = drake::readd(plot_tradeoff_10)
)
```

```{r}
girafe(
    ggobj = drake::readd(plot_tradeoff_100)
)
```

As expected, `ChenEtAl` is the slowest algorithm and, somewhat surprisingly, is not the most accurate one,
even though all coreset-based algorithms use it as a subroutine to compute their output.
The other surprising thing is the performance of `Random`.
While it is obvious that it should be the fastest, since it does very little work, the quality of
random clusterings is surprisingly good.
So good, in fact, that it outperforms some configurations of the coreset-based algorithms, especially
on the full dataset under euclidean distance.
One can easily see that an heuristic that simply draws some random independent sets and then keeps the maximum 
would get very good results, even though with no guarantees.

As for the coreset-based approaches, we have that `SeqCoreset` and `MRCoreset` provide solutions of comparable quality

One could also think about drawing some random independent sets on the coresets and compare their radius with the
one obtained with `ChenEtAl`, keeping the best.

## Time breakdown

```{r fig.height=4}
drake::readd(plot_time_10)
```

```{r fig.height=4}
drake::readd(plot_time_100)
```

## Time for building the coreset only

```{r fig.height=4}
drake::readd(plot_time_coreset_10)
```

```{r fig.height=4}
drake::readd(plot_time_coreset_100)
```

